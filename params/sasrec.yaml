# Parameters for SASRec training
mode: train
dataset_code: amazon
amazon_url: https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/categoryFiles/AMAZON_FASHION.json.gz
amazon_metadata_url: https://mcauleylab.ucsd.edu/public_datasets/data/amazon_v2/metaFiles2/meta_AMAZON_FASHION.json.gz
min_rating: 0
min_uc: 5
min_sc: 0
split: leave_one_out

dataloader_code: sasrec
train_batch_size: 128
val_batch_size: 128
test_batch_size: 128

train_negative_sampler_code: random
train_negative_sample_size: 64 # Used in trainer
train_negative_sampling_seed: 0
test_negative_sampler_code: random
test_negative_sample_size: 100
test_negative_sampling_seed: 98765

trainer_code: sasrec
use_hard_negative_mining: false
use_prefix_augmentation: false
device: cuda
num_gpu: 1
device_idx: '0'
optimizer: Adam
lr: 0.001
enable_lr_schedule: false
decay_step: 10
gamma: 1.0
num_epochs: 50
metric_ks: [1, 5, 10, 20, 50, 100]
best_metric: "NDCG@10"

model_code: sasrec
model_init_seed: 0

bert_dropout: 0.1
bert_hidden_units: 256
bert_max_len: 100
bert_num_blocks: 2
bert_num_heads: 4
infonce_temperature: 0.07

do_test: true

experiment_description: "sasrec_emb_test"
generate_item_embeddings: false
item_embedding_path: "Data/preprocessed/amazon_min_rating0-min_uc5-min_sc0-splitleave_one_out/item_embeddings.npy"
projection_mlp_dims: [256]
projection_dropout: 0.0
recreate_data: false

# For gBCE loss
loss_type: gbce
gbce_q: 0.5
